{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKeOyvcdd2cTIJB9cs7Qzb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VISH101-vue/SPAM-HAM-DETECTION/blob/main/Spam%20Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ4PS85Q0nTT",
        "outputId": "dc97f6f1-4f31-42cd-8fde-140d313cf082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "nWBIvfNAaqg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"SpamCommentsAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "file_loc = \"/content/Youtube02-KatyPerry.csv\"  # Update with the correct local path\n",
        "dataf = spark.read.csv(file_loc, header=True, inferSchema=True)\n",
        "\n",
        "dataf.printSchema()\n",
        "dataf.show(5)\n",
        "\n",
        "# Drop the unwanted columns\n",
        "columns_to_drop = [\"COMMENT_ID\", \"DATE\"]\n",
        "dataf = dataf.drop(*columns_to_drop)\n",
        "\n",
        "dataf.printSchema()\n",
        "\n",
        "# Separate the DataFrame into two based on \"Spam Indicator\" values\n",
        "ham_dataf = dataf.filter(col(\"CLASS\") == 0)\n",
        "spam_dataf = dataf.filter(col(\"CLASS\") == 1)\n",
        "\n",
        "total_ham_count = ham_dataf.count()\n",
        "total_spam_count = spam_dataf.count()\n",
        "ham_dataf.show()\n",
        "spam_dataf.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FCfFEGyd3qtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ff9cf0-a875-401e-c223-5722aa0d3a9d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- COMMENT_ID: string (nullable = true)\n",
            " |-- AUTHOR: string (nullable = true)\n",
            " |-- DATE: timestamp (nullable = true)\n",
            " |-- CONTENT: string (nullable = true)\n",
            " |-- CLASS: string (nullable = true)\n",
            "\n",
            "+--------------------+------------+-------------------+--------------------+-----+\n",
            "|          COMMENT_ID|      AUTHOR|               DATE|             CONTENT|CLASS|\n",
            "+--------------------+------------+-------------------+--------------------+-----+\n",
            "|z12pgdhovmrktzm3i...| lekanaVEVO1|2014-07-22 15:27:50|i love this so mu...|    1|\n",
            "|z13yx345uxepetggz...|    Pyunghee|2014-07-27 01:57:16|http://www.billbo...|    1|\n",
            "|z12lsjvi3wa5x1vwh...|  Erica Ross|2014-07-27 02:51:43|Hey guys! Please ...|    1|\n",
            "|z13jcjuovxbwfr0ge...|Aviel Haimov|2014-08-01 12:27:48|http://psnboss.co...|    1|\n",
            "|z13qybua2yfydzxzj...|  John Bello|2014-08-01 21:04:03|Hey everyone. Wat...|    1|\n",
            "+--------------------+------------+-------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- AUTHOR: string (nullable = true)\n",
            " |-- CONTENT: string (nullable = true)\n",
            " |-- CLASS: string (nullable = true)\n",
            "\n",
            "+--------------------+--------------------+-----+\n",
            "|              AUTHOR|             CONTENT|CLASS|\n",
            "+--------------------+--------------------+-----+\n",
            "|         Daniel Korp|katy perry does r...|    0|\n",
            "|         Paul Hannam|In what South Ame...|    0|\n",
            "|     Angie Sivrikozi|Its a good song a...|    0|\n",
            "|         Zain Hassan|Thanks to this vi...|    0|\n",
            "|           Sam Klein|She named the tig...|    0|\n",
            "|      Littleprincess|     Perfect! &lt;3Ôªø|    0|\n",
            "|        Justin Chery|And after the vid...|    0|\n",
            "|     Jack El Matador|should not have p...|    0|\n",
            "|           xhonavsky|\"I love this song...|    0|\n",
            "|         Ricky Smith|This song makes m...|    0|\n",
            "|       maurice davis|If she really did...|    0|\n",
            "|          Robert Kim|I'm sorry Katy Pe...|    0|\n",
            "|     tombraiderxXx12|I'm not a big fan...|    0|\n",
            "|      Eder Rodriguez|katy perry  just ...|    0|\n",
            "|            Kathy L.|In my opinion I t...|    0|\n",
            "|   Sofia Aristizabal|I loved, she is a...|    0|\n",
            "|        Rahul Thorat|You gonna hear me...|    0|\n",
            "|     Christ Is Risen|I WILL FINISH THI...|    0|\n",
            "|blondedbythelight703|She's an old Whore!Ôªø|    0|\n",
            "|          moilovemoi|ROAAAAARRRRRR üêØ?...|    0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------------+--------------------+-----+\n",
            "|          AUTHOR|             CONTENT|CLASS|\n",
            "+----------------+--------------------+-----+\n",
            "|     lekanaVEVO1|i love this so mu...|    1|\n",
            "|        Pyunghee|http://www.billbo...|    1|\n",
            "|      Erica Ross|Hey guys! Please ...|    1|\n",
            "|    Aviel Haimov|http://psnboss.co...|    1|\n",
            "|      John Bello|Hey everyone. Wat...|    1|\n",
            "|Nere Overstylish|check out my rapp...|    1|\n",
            "|         Jayki L|Subscribe pleaaaa...|    1|\n",
            "|          djh3mi|hey guys!! visit ...|    1|\n",
            "|    Manuel Ortiz|Nice! http://www....|    1|\n",
            "|    Mike Bennett|http://www.twitch...|    1|\n",
            "|       Lil Misme|Hey Guys this is ...|    1|\n",
            "|          Emilie|Hey guys! My mom ...|    1|\n",
            "| Eduarda Ketrony|https://www.faceb...|    1|\n",
            "|    Jennika Chua|https://www.faceb...|    1|\n",
            "|          Artady|https://soundclou...|    1|\n",
            "|      J STROMMER|Take a break from...|    1|\n",
            "|   Evin Alshamas|Subscribe me pleaseÔªø|    1|\n",
            "|      maddog1431|Check out my cove...|    1|\n",
            "|         MB LOVE|Subscribe to My C...|    1|\n",
            "|      IbbyUchiha|Hey guys. I am a ...|    1|\n",
            "+----------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ham_dataf = ham_dataf.filter(ham_dataf.CONTENT.isNotNull())\n",
        "spam_dataf = spam_dataf.filter(spam_dataf.CONTENT.isNotNull())\n"
      ],
      "metadata": {
        "id": "u302HMhm8VQM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import lower,col\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(inputCol=\"CONTENT\", outputCol=\"text\")\n",
        "\n",
        "# Remove stopwords\n",
        "remove = StopWordsRemover(inputCol=\"text\", outputCol=\"processed_text\")\n",
        "\n",
        "# Convert text to lowercase\n",
        "spam_dataf = spam_dataf.withColumn(\"processed_message\", lower(col(\"CONTENT\")))\n",
        "\n",
        "spam_pipeline = Pipeline(stages=[tokenizer, remove])\n",
        "spam_temp = spam_pipeline.fit(spam_dataf)\n",
        "preprocessed_spam_data = spam_temp.transform(spam_dataf)"
      ],
      "metadata": {
        "id": "3LaW9k7lkVFO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import lower,col\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(inputCol=\"CONTENT\", outputCol=\"text\")\n",
        "\n",
        "# Remove stopwords\n",
        "remove = StopWordsRemover(inputCol=\"text\", outputCol=\"processed_text\")\n",
        "\n",
        "# Convert text to lowercase\n",
        "ham_dataf = ham_dataf.withColumn(\"processed_message\", lower(col(\"CONTENT\")))\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "\n",
        "\n",
        "ham_pipeline = Pipeline(stages=[tokenizer, remove])\n",
        "ham_temp = ham_pipeline.fit(ham_dataf)\n",
        "preprocessed_ham_data = ham_temp.transform(ham_dataf)"
      ],
      "metadata": {
        "id": "NfTNrGfD6Jni"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_ham_data.show(5)"
      ],
      "metadata": {
        "id": "bXEUKr0UiEMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6eac35-e799-4f79-9f22-595668c37788"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "|         AUTHOR|             CONTENT|CLASS|   processed_message|                text|      processed_text|\n",
            "+---------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "|    Daniel Korp|katy perry does r...|    0|katy perry does r...|[katy, perry, doe...|[katy, perry, rem...|\n",
            "|    Paul Hannam|In what South Ame...|    0|in what south ame...|[in, what, south,...|[south, american,...|\n",
            "|Angie Sivrikozi|Its a good song a...|    0|its a good song a...|[its, a, good, so...|[good, song, like...|\n",
            "|    Zain Hassan|Thanks to this vi...|    0|thanks to this vi...|[thanks, to, this...|[thanks, video, k...|\n",
            "|      Sam Klein|She named the tig...|    0|she named the tig...|[she, named, the,...|[named, tiger, ki...|\n",
            "+---------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_spam_data.show(5)"
      ],
      "metadata": {
        "id": "TRKZUsTNd1nU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9fdb92-478d-447a-ccb0-7ef5e1641904"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "|      AUTHOR|             CONTENT|CLASS|   processed_message|                text|      processed_text|\n",
            "+------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "| lekanaVEVO1|i love this so mu...|    1|i love this so mu...|[i, love, this, s...|[love, much., als...|\n",
            "|    Pyunghee|http://www.billbo...|    1|http://www.billbo...|[http://www.billb...|[http://www.billb...|\n",
            "|  Erica Ross|Hey guys! Please ...|    1|hey guys! please ...|[hey, guys!, plea...|[hey, guys!, plea...|\n",
            "|Aviel Haimov|http://psnboss.co...|    1|http://psnboss.co...|[http://psnboss.c...|[http://psnboss.c...|\n",
            "|  John Bello|Hey everyone. Wat...|    1|hey everyone. wat...|[hey, everyone., ...|[hey, everyone., ...|\n",
            "+------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spam_texts = preprocessed_spam_data.filter(col(\"CLASS\") == \"1\").select(\"processed_text\").rdd.flatMap(lambda x: x).collect()\n",
        "ham_texts = preprocessed_ham_data.filter(col(\"CLASS\") == \"0\").select(\"processed_text\").rdd.flatMap(lambda x: x).collect()"
      ],
      "metadata": {
        "id": "hNrQKoWuKl3S"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_texts_flat = [i for l in spam_texts for i in l]\n",
        "ham_texts_flat = [i for l in ham_texts for i in l]\n"
      ],
      "metadata": {
        "id": "y-5OIoriK1p8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "spam_freq = nltk.FreqDist(spam_texts_flat)\n",
        "spam_freq.most_common(10)\n",
        "ham_freq = nltk.FreqDist(ham_texts_flat)\n",
        "ham_freq.most_common(10)\n"
      ],
      "metadata": {
        "id": "Zt-ioeCBNCeC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6533c2b6-27ca-4511-c9fc-a3a6d70774d0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 170),\n",
              " ('katy', 49),\n",
              " ('perry', 30),\n",
              " ('love', 30),\n",
              " ('song', 29),\n",
              " ('like', 24),\n",
              " ('video', 22),\n",
              " ('\\ufeff', 18),\n",
              " ('good', 11),\n",
              " ('roar', 11)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr, when\n",
        "\n",
        "# Assuming preprocessed_spam_data is your DataFrame\n",
        "top_10_spam_words = [item[0].strip(\"',\") for item in spam_freq.most_common(10)]\n",
        "output_spam_dataf = preprocessed_spam_data\n",
        "\n",
        "# Define the conditions for each word dynamically\n",
        "conditions = [expr(f\"array_contains(text, '{word}')\") for word in top_10_spam_words]\n",
        "\n",
        "# Create a \"result\" column indicating whether any of the top 10 spam words are present\n",
        "output_spam_dataf = output_spam_dataf.withColumn(\"result\", when(conditions[0], \"Yes\"))\n",
        "\n",
        "# Loop through the remaining top 10 spam words and update the \"result\" column\n",
        "for i in range(1, len(conditions)):\n",
        "    output_spam_dataf = output_spam_dataf.withColumn(\"result\", when(conditions[i], \"Yes\").otherwise(col(\"result\")))\n",
        "\n",
        "# Display the rows where any of the top 10 spam words are present\n",
        "output_spam_dataf.filter(col(\"result\") == \"Yes\").show(10)\n",
        "spam_filtered_data = output_spam_dataf.filter(col(\"result\") == \"Yes\")\n",
        "\n",
        "# Select specific columns (\"AUTHOR\" and \"CLASS\")\n",
        "selected_columns = spam_filtered_data.select(\"AUTHOR\", \"CLASS\")\n",
        "\n",
        "# Show the combined result\n",
        "selected_columns.show(10)\n"
      ],
      "metadata": {
        "id": "3E4zXevNMeww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7f4303-55b0-4978-f6c1-e9c4c9ce70a9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------------+-----+--------------------+--------------------+--------------------+------+\n",
            "|          AUTHOR|             CONTENT|CLASS|   processed_message|                text|      processed_text|result|\n",
            "+----------------+--------------------+-----+--------------------+--------------------+--------------------+------+\n",
            "|     lekanaVEVO1|i love this so mu...|    1|i love this so mu...|[i, love, this, s...|[love, much., als...|   Yes|\n",
            "|      Erica Ross|Hey guys! Please ...|    1|hey guys! please ...|[hey, guys!, plea...|[hey, guys!, plea...|   Yes|\n",
            "|      John Bello|Hey everyone. Wat...|    1|hey everyone. wat...|[hey, everyone., ...|[hey, everyone., ...|   Yes|\n",
            "|Nere Overstylish|check out my rapp...|    1|check out my rapp...|[check, out, my, ...|[check, rapping, ...|   Yes|\n",
            "|         Jayki L|Subscribe pleaaaa...|    1|subscribe pleaaaa...|[subscribe, pleaa...|[subscribe, pleaa...|   Yes|\n",
            "|          djh3mi|hey guys!! visit ...|    1|hey guys!! visit ...|[hey, guys!!, vis...|[hey, guys!!, vis...|   Yes|\n",
            "|    Manuel Ortiz|Nice! http://www....|    1|nice! http://www....|[nice!, http://ww...|[nice!, http://ww...|   Yes|\n",
            "|       Lil Misme|Hey Guys this is ...|    1|hey guys this is ...|[hey, guys, this,...|[hey, guys, glamo...|   Yes|\n",
            "|          Emilie|Hey guys! My mom ...|    1|hey guys! my mom ...|[hey, guys!, my, ...|[hey, guys!, mom,...|   Yes|\n",
            "|    Jennika Chua|https://www.faceb...|    1|https://www.faceb...|[https://www.face...|[https://www.face...|   Yes|\n",
            "+----------------+--------------------+-----+--------------------+--------------------+--------------------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------------+-----+\n",
            "|          AUTHOR|CLASS|\n",
            "+----------------+-----+\n",
            "|     lekanaVEVO1|    1|\n",
            "|      Erica Ross|    1|\n",
            "|      John Bello|    1|\n",
            "|Nere Overstylish|    1|\n",
            "|         Jayki L|    1|\n",
            "|          djh3mi|    1|\n",
            "|    Manuel Ortiz|    1|\n",
            "|       Lil Misme|    1|\n",
            "|          Emilie|    1|\n",
            "|    Jennika Chua|    1|\n",
            "+----------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr\n",
        "\n",
        "# Assuming preprocessed_spam_data is your DataFrame\n",
        "top_10_ham_words = [item[0].strip(\"',\") for item in ham_freq.most_common(10)]\n",
        "output_ham_dataf = preprocessed_ham_data\n",
        "\n",
        "# Define the conditions for each word dynamically\n",
        "conditions = [expr(f\"array_contains(text, '{word}')\") for word in top_10_ham_words]\n",
        "\n",
        "# Create a \"result\" column indicating whether any of the top 10 spam words are present\n",
        "output_ham_dataf = output_ham_dataf.withColumn(\"result\", when(conditions[0], \"Yes\"))\n",
        "\n",
        "# Loop through the remaining top 10 spam words and update the \"result\" column\n",
        "for i in range(1, len(conditions)):\n",
        "    output_ham_dataf = output_ham_dataf.withColumn(\"result\", when(conditions[i], \"Yes\").otherwise(col(\"result\")))\n",
        "\n",
        "# Display the rows where any of the top 10 spam words are present\n",
        "output_ham_dataf.filter(col(\"result\") == \"Yes\").show(10)\n",
        "\n",
        "# Filter rows where \"result\" is \"Yes\"\n",
        "ham_filtered_data = output_ham_dataf.filter(col(\"result\") == \"Yes\")\n",
        "\n",
        "# Select specific columns (\"AUTHOR\" and \"CLASS\")\n",
        "selected_columns = ham_filtered_data.select(\"AUTHOR\", \"CLASS\")\n",
        "\n",
        "# Show the combined result\n",
        "selected_columns.show(10)\n"
      ],
      "metadata": {
        "id": "F5_HCt6tT51K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a856198-3190-4116-d545-0fc1764d9484"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+-----+--------------------+--------------------+--------------------+------+\n",
            "|         AUTHOR|             CONTENT|CLASS|   processed_message|                text|      processed_text|result|\n",
            "+---------------+--------------------+-----+--------------------+--------------------+--------------------+------+\n",
            "|    Daniel Korp|katy perry does r...|    0|katy perry does r...|[katy, perry, doe...|[katy, perry, rem...|   Yes|\n",
            "|    Paul Hannam|In what South Ame...|    0|in what south ame...|[in, what, south,...|[south, american,...|   Yes|\n",
            "|Angie Sivrikozi|Its a good song a...|    0|its a good song a...|[its, a, good, so...|[good, song, like...|   Yes|\n",
            "|    Zain Hassan|Thanks to this vi...|    0|thanks to this vi...|[thanks, to, this...|[thanks, video, k...|   Yes|\n",
            "|      Sam Klein|She named the tig...|    0|she named the tig...|[she, named, the,...|[named, tiger, ki...|   Yes|\n",
            "|   Justin Chery|And after the vid...|    0|and after the vid...|[and, after, the,...|[video, ends,, 13...|   Yes|\n",
            "|      xhonavsky|\"I love this song...|    0|\"i love this song...|[\"i, love, this, ...|[\"i, love, song,,...|   Yes|\n",
            "|    Ricky Smith|This song makes m...|    0|this song makes m...|[this, song, make...|[song, makes, wan...|   Yes|\n",
            "|     Robert Kim|I'm sorry Katy Pe...|    0|i'm sorry katy pe...|[i'm, sorry, katy...|[sorry, katy, per...|   Yes|\n",
            "|tombraiderxXx12|I'm not a big fan...|    0|i'm not a big fan...|[i'm, not, a, big...|[big, fan, song, ...|   Yes|\n",
            "+---------------+--------------------+-----+--------------------+--------------------+--------------------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+---------------+-----+\n",
            "|         AUTHOR|CLASS|\n",
            "+---------------+-----+\n",
            "|    Daniel Korp|    0|\n",
            "|    Paul Hannam|    0|\n",
            "|Angie Sivrikozi|    0|\n",
            "|    Zain Hassan|    0|\n",
            "|      Sam Klein|    0|\n",
            "|   Justin Chery|    0|\n",
            "|      xhonavsky|    0|\n",
            "|    Ricky Smith|    0|\n",
            "|     Robert Kim|    0|\n",
            "|tombraiderxXx12|    0|\n",
            "+---------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
        "from math import log\n",
        "\n",
        "# Create a DataFrame`\n",
        "dataf = spam_filtered_data.select(\"AUTHOR\", \"CONTENT\")\n",
        "\n",
        "# Tokenize the text column\n",
        "tokenizer_udf = udf(lambda text: text.split(), ArrayType(StringType()))\n",
        "dataf = dataf.withColumn(\"text\", tokenizer_udf(dataf[\"CONTENT\"]))\n",
        "\n",
        "# Calculate Term Frequencies (TF)\n",
        "def calculate_tf(text_list):\n",
        "    total_count = {}\n",
        "    total_words = len(text_list)\n",
        "    for word in text_list:\n",
        "        total_count[word] = total_count.get(word, 0) + 1\n",
        "    return {k: v / total_words for k, v in total_count.items()}\n",
        "\n",
        "calculate_tf_udf = udf(calculate_tf, StringType())\n",
        "dataf = dataf.withColumn(\"tf\", calculate_tf_udf(dataf[\"text\"]))\n",
        "\n",
        "# Create a list of all unique words in the documents\n",
        "unique_words = list(set(dataf.selectExpr(\"explode(text) as word\").select(\"word\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
        "\n",
        "# Calculate Inverse Document Frequencies (IDF)\n",
        "total_documents = dataf.count()\n",
        "\n",
        "# Calculate document frequency (DF)\n",
        "document_frequency = dataf.select(\"AUTHOR\", \"text\").rdd.flatMap(lambda x: [(word, 1) for word in set(x[1])]).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "idf_values = document_frequency.map(lambda x: (x[0], log(total_documents / x[1])))\n",
        "\n",
        "# Broadcast IDF values\n",
        "idf_broadcast = spark.sparkContext.broadcast(dict(idf_values.collect()))\n",
        "\n",
        "# Calculate TF-IDF\n",
        "def calculate_tfidf(row):\n",
        "    user_name, words = row\n",
        "    tfidf_values = {word: word.count(word) * idf_broadcast.value.get(word, 0.0) for word in words}\n",
        "    return user_name, words, tfidf_values\n",
        "\n",
        "tfidf_data = dataf.select(\"AUTHOR\", \"text\").rdd.map(calculate_tfidf)\n",
        "\n",
        "# Display the result\n",
        "tfidf_df = spark.createDataFrame(tfidf_data, [\"AUTHOR\", \"text\", \"tfidf\"])\n",
        "tfidf_df.show(10)"
      ],
      "metadata": {
        "id": "N3YkGqlIEP_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362b3926-77fb-46b2-a7ca-ed866cd75dd2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------------+--------------------+\n",
            "|          AUTHOR|                text|               tfidf|\n",
            "+----------------+--------------------+--------------------+\n",
            "|     lekanaVEVO1|[i, love, this, s...|{Pilot -> 4.88280...|\n",
            "|      Erica Ross|[Hey, guys!, Plea...|{All -> 4.8828019...|\n",
            "|      John Bello|[Hey, everyone., ...|{http://believeme...|\n",
            "|Nere Overstylish|[check, out, my, ...|{https://soundclo...|\n",
            "|         Jayki L|[Subscribe, pleaa...|{‚ô• -> 4.189654742...|\n",
            "|          djh3mi|[hey, guys!!, vis...|{a -> 1.356441397...|\n",
            "|    Manuel Ortiz|[Nice!, http://ww...|{Nice! -> 4.88280...|\n",
            "|       Lil Misme|[Hey, Guys, this,...|{youtube -> 3.496...|\n",
            "|          Emilie|[Hey, guys!, My, ...|{no -> 3.09104245...|\n",
            "|    Jennika Chua|[https://www.face...|{https://www.face...|\n",
            "+----------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
        "from math import log\n",
        "\n",
        "# Create a DataFrame`\n",
        "dataf = ham_filtered_data.select(\"AUTHOR\", \"CONTENT\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer_udf = udf(lambda text: text.split(), ArrayType(StringType()))\n",
        "dataf = dataf.withColumn(\"text\", tokenizer_udf(dataf[\"CONTENT\"]))\n",
        "\n",
        "# Calculate Term Frequencies (TF)\n",
        "def calculate_tf(text_list):\n",
        "    total_count = {}\n",
        "    total_words = len(text_list)\n",
        "    for word in text_list:\n",
        "        total_count[word] = total_count.get(word, 0) + 1\n",
        "    return {k: v / total_words for k, v in total_count.items()}\n",
        "\n",
        "calculate_tf_udf = udf(calculate_tf, StringType())\n",
        "dataf = dataf.withColumn(\"tf\", calculate_tf_udf(dataf[\"text\"]))\n",
        "\n",
        "# Create a list of all unique words\n",
        "unique_words = list(set(dataf.selectExpr(\"explode(text) as word\").select(\"word\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
        "\n",
        "# Calculate Inverse Document Frequencies (IDF)\n",
        "total_documents = dataf.count()\n",
        "\n",
        "# Calculate document frequency (DF) for\n",
        "document_frequency = dataf.select(\"AUTHOR\", \"text\").rdd.flatMap(lambda x: [(word, 1) for word in set(x[1])]).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "idf_values = document_frequency.map(lambda x: (x[0], log(total_documents / x[1])))\n",
        "\n",
        "# Broadcast IDF values\n",
        "idf_broadcast = spark.sparkContext.broadcast(dict(idf_values.collect()))\n",
        "\n",
        "# Calculate TF-IDF\n",
        "def calculate_tfidf(row):\n",
        "    user_name, words = row\n",
        "    tfidf_values = {word: word.count(word) * idf_broadcast.value.get(word, 0.0) for word in words}\n",
        "    return user_name, words, tfidf_values\n",
        "\n",
        "tfidf_data = dataf.select(\"AUTHOR\", \"text\").rdd.map(calculate_tfidf)\n",
        "\n",
        "# Display the result\n",
        "tfidf_dataf = spark.createDataFrame(tfidf_data, [\"AUTHOR\", \"text\", \"tfidf\"])\n",
        "tfidf_dataf.show(10)"
      ],
      "metadata": {
        "id": "LAO3R7WKVvRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d2b0f34-34c8-4cec-9734-0c717f9b134d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------------------+\n",
            "|         AUTHOR|                text|               tfidf|\n",
            "+---------------+--------------------+--------------------+\n",
            "|    Daniel Korp|[katy, perry, doe...|{a -> 1.524444699...|\n",
            "|    Paul Hannam|[In, what, South,...|{American -> 4.12...|\n",
            "|Angie Sivrikozi|[Its, a, good, so...|{love -> 1.684787...|\n",
            "|    Zain Hassan|[Thanks, to, this...|{plane -> 4.12713...|\n",
            "|      Sam Klein|[She, named, the,...|{did, -> 4.820281...|\n",
            "|   Justin Chery|[And, after, the,...|{ft. -> 4.8202815...|\n",
            "|      xhonavsky|[\"I, love, this, ...|{love -> 1.684787...|\n",
            "|    Ricky Smith|[This, song, make...|{song -> 1.642227...|\n",
            "|     Robert Kim|[I'm, sorry, Katy...|{love -> 1.684787...|\n",
            "|tombraiderxXx12|[I'm, not, a, big...|{but -> 2.8743714...|\n",
            "+---------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Stop the Spark session\n",
        "# spark.stop()"
      ],
      "metadata": {
        "id": "mFPm2e7uPSo1"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}